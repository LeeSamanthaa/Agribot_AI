{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGJXY5tnOLHS"
      },
      "source": [
        "#Merge and Override of final_lstm_dataset_cleaned.parquet\n",
        "\n",
        "\n",
        "- Merging previous parquet with zeroshot_ground_truth.parquet.\n",
        "- Look at previous code for pre-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "merge_datasets.py\n",
        "Author: Samantha Lee\n",
        "\n",
        "Purpose: Merge \"Zero-Shot\" data (with coords) and \"Budapest/Vas\" data.\n",
        "Crucially, it injects missing coordinates for Vas/Budapest so the map works.\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Adjust paths relative to where you run the script (project root)\n",
        "DATA_DIR = Path(\"data\")\n",
        "\n",
        "# 1. The file with coordinates (from pre-processing)\n",
        "# If this file doesn't exist, the script will warn but proceed with just the manual fallback\n",
        "ZEROSHOT_PATH = DATA_DIR / \"zeroshot_ground_truth_with_coords.parquet\"\n",
        "\n",
        "# 2. The file with Vas/Budapest data (YOUR SPECIFIC FILE)\n",
        "# I am looking for the file you uploaded. Ensure it is renamed to this:\n",
        "BUDAPEST_VAS_PATH = DATA_DIR / \"budapest_vas.parquet\"\n",
        "\n",
        "# 3. The Output File (The one the App reads)\n",
        "OUTPUT_PATH = DATA_DIR / \"final_lstm_dataset_cleaned.parquet\"\n",
        "\n",
        "# --- MANUAL COORDINATE FALLBACK ---\n",
        "# Coordinates for regions likely in the Budapest/Vas file but missing lat/lon columns\n",
        "# These act as a safety net.\n",
        "MANUAL_COORDS = {\n",
        "    \"Budapest\": {\"lat\": 47.4979, \"lon\": 19.0402},\n",
        "    \"Vas\": {\"lat\": 47.0353, \"lon\": 16.7665},\n",
        "    \"Fejer\": {\"lat\": 47.1625, \"lon\": 18.4048},\n",
        "    \"Baranya\": {\"lat\": 46.0727, \"lon\": 18.2323},\n",
        "    \"Pest\": {\"lat\": 47.4167, \"lon\": 19.3333},\n",
        "    \"Tolna\": {\"lat\": 46.5000, \"lon\": 18.5000},\n",
        "    \"Gyor\": {\"lat\": 47.6875, \"lon\": 17.6504},\n",
        "    \"Somogy\": {\"lat\": 46.5000, \"lon\": 17.5000},\n",
        "    \"Zala\": {\"lat\": 46.7500, \"lon\": 16.8333},\n",
        "    \"Heves\": {\"lat\": 47.9026, \"lon\": 20.3733},\n",
        "    \"Borsod\": {\"lat\": 48.1000, \"lon\": 20.7833},\n",
        "    \"Szabolcs\": {\"lat\": 48.0000, \"lon\": 22.0000},\n",
        "    \"Hajdu\": {\"lat\": 47.5316, \"lon\": 21.6273},\n",
        "    \"Bekes\": {\"lat\": 46.6736, \"lon\": 21.0737},\n",
        "    \"Csongrad\": {\"lat\": 46.2530, \"lon\": 20.1482},\n",
        "    \"Bacs\": {\"lat\": 46.5000, \"lon\": 19.5000},\n",
        "    \"Jasz\": {\"lat\": 47.5000, \"lon\": 20.0000},\n",
        "    \"Nograd\": {\"lat\": 48.0000, \"lon\": 19.5000},\n",
        "    \"Komarom\": {\"lat\": 47.5862, \"lon\": 18.0172},\n",
        "    \"Veszprem\": {\"lat\": 47.1000, \"lon\": 17.9000}\n",
        "}\n",
        "\n",
        "print(\"--- STARTING CUSTOM MERGE ---\")\n",
        "\n",
        "# 1. Load Zero-Shot Data (The Coordinate Source)\n",
        "df_zero = pd.DataFrame()\n",
        "if ZEROSHOT_PATH.exists():\n",
        "    print(f\"Loading Zero-Shot data: {ZEROSHOT_PATH.name}\")\n",
        "    try:\n",
        "        df_zero = pd.read_parquet(ZEROSHOT_PATH, engine='fastparquet')\n",
        "        print(f\"   Loaded {len(df_zero)} rows with coordinates.\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Error loading zero-shot data: {e}. Proceeding without it.\")\n",
        "else:\n",
        "    print(f\"Warning: {ZEROSHOT_PATH.name} not found. Will rely on manual coordinates.\")\n",
        "\n",
        "# 2. Load Budapest/Vas Data (The Historical Data)\n",
        "if not BUDAPEST_VAS_PATH.exists():\n",
        "    # Fallback: Check if the user hasn't renamed it yet\n",
        "    possible_name = DATA_DIR / \"final_lstm_dataset_cleaned budapest_vas.parquet\"\n",
        "    if possible_name.exists():\n",
        "        print(f\"Found file with original name: {possible_name.name}\")\n",
        "        BUDAPEST_VAS_PATH = possible_name\n",
        "    else:\n",
        "        print(f\"CRITICAL ERROR: Could not find 'budapest_vas.parquet' in {DATA_DIR}\")\n",
        "        print(\"Please rename your uploaded file to 'budapest_vas.parquet' and place it in the 'data' folder.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "print(f\"Loading Budapest/Vas data: {BUDAPEST_VAS_PATH.name}\")\n",
        "try:\n",
        "    df_bv = pd.read_parquet(BUDAPEST_VAS_PATH, engine='fastparquet')\n",
        "    print(f\"   Loaded {len(df_bv)} historical rows.\")\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR reading parquet file: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# 3. Fix Coordinates for Budapest/Vas\n",
        "print(\"Backfilling coordinates for Budapest/Vas data...\")\n",
        "\n",
        "# Ensure columns exist\n",
        "if 'latitude' not in df_bv.columns: df_bv['latitude'] = np.nan\n",
        "if 'longitude' not in df_bv.columns: df_bv['longitude'] = np.nan\n",
        "\n",
        "def get_coords(field_id):\n",
        "    # Helper to find coordinates based on the region name inside the field_id\n",
        "    # Format example: \"vtx|Vas|rapeseed...\"\n",
        "    try:\n",
        "        # Check against our manual list\n",
        "        for key, coords in MANUAL_COORDS.items():\n",
        "            # Case-insensitive check\n",
        "            if f\"|{key}|\" in field_id or f\"|{key.lower()}|\" in field_id.lower():\n",
        "                return coords['lat'], coords['lon']\n",
        "    except:\n",
        "        pass\n",
        "    return np.nan, np.nan\n",
        "\n",
        "# Only apply to rows missing coordinates\n",
        "# This is efficient: we only process the rows that need it.\n",
        "mask_missing = df_bv['latitude'].isna()\n",
        "missing_count = mask_missing.sum()\n",
        "\n",
        "if missing_count > 0:\n",
        "    print(f\"   Found {missing_count} rows missing coords. Applying manual fix...\")\n",
        "    \n",
        "    # Get unique field_ids that are missing coords to speed up lookup\n",
        "    unique_missing_ids = df_bv.loc[mask_missing, 'field_id'].unique()\n",
        "    print(f\"   Unique fields to fix: {len(unique_missing_ids)}\")\n",
        "    \n",
        "    # Create a map for these specific IDs\n",
        "    id_to_coords = {fid: get_coords(fid) for fid in unique_missing_ids}\n",
        "    \n",
        "    # Apply map\n",
        "    coords_series = df_bv.loc[mask_missing, 'field_id'].map(id_to_coords)\n",
        "    \n",
        "    # Assign back\n",
        "    df_bv.loc[mask_missing, 'latitude'] = coords_series.apply(lambda x: x[0])\n",
        "    df_bv.loc[mask_missing, 'longitude'] = coords_series.apply(lambda x: x[1])\n",
        "    \n",
        "    # Check results\n",
        "    still_missing = df_bv['latitude'].isna().sum()\n",
        "    print(f\"   Fixed {missing_count - still_missing} rows.\")\n",
        "    if still_missing > 0:\n",
        "        print(f\"   Warning: {still_missing} rows still lack coordinates (Region name not in manual list).\")\n",
        "else:\n",
        "    print(\"   All rows already have coordinates!\")\n",
        "\n",
        "# 4. Merge\n",
        "print(\"Merging datasets...\")\n",
        "# We stack them. If zero-shot has newer data for the same field/date, we keep it.\n",
        "df_final = pd.concat([df_zero, df_bv], ignore_index=True)\n",
        "df_final = df_final.drop_duplicates(subset=['field_id', 'date'], keep='last')\n",
        "\n",
        "# 5. Save\n",
        "print(f\"Saving FINAL merged dataset ({len(df_final)} rows) to: {OUTPUT_PATH}\")\n",
        "df_final.to_parquet(OUTPUT_PATH, engine='fastparquet', index=False)\n",
        "\n",
        "print(\"--- SUCCESS! Vas and Budapest are restored with coordinates. ---\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
