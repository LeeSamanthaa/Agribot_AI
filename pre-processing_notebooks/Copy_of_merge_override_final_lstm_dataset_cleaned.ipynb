{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Merge and Override of final_lstm_dataset_cleaned.parquet\n",
        "\n",
        "\n",
        "- Merging previous parquet with zeroshot_ground_truth.parquet. (with coordinates)\n",
        "- Look at previous code for pre-processing\n"
      ],
      "metadata": {
        "id": "TGJXY5tnOLHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastparquet pyarrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3PLAmPerXgU",
        "outputId": "63f02d12-842c-4bf2-8577-2a073b8dc1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.0.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.11.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from fastparquet) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n",
            "Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastparquet\n",
            "Successfully installed fastparquet-2024.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "merge_datasets.py\n",
        "Author: Samantha Lee\n",
        "\n",
        "This script merges the old 'final_lstm_dataset_cleaned.parquet' (original data)\n",
        "with the new 'zeroshot_ground_truth_with_coords.parquet' (new data/coordinates)\n",
        "and overwrites the original file after deduplication.\n",
        "\n",
        "FIX: Ensures all records in the combined output have 'latitude' and 'longitude'\n",
        "by using the new data as a lookup source for coordinates.\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "# Install fastparquet if not already installed\n",
        "try:\n",
        "    import fastparquet\n",
        "except ImportError:\n",
        "    print(\"fastparquet not found, installing...\")\n",
        "    %pip install fastparquet\n",
        "    print(\"fastparquet installed. Please re-run the cell.\")\n",
        "    sys.exit(1) # Exit to force re-run after installation\n",
        "\n",
        "# --- Path Setup ---\n",
        "DATA_DIR = Path(\".\") # Changed from Path(\"data\") to Path(\".\")\n",
        "\n",
        "# --- Define File Names ---\n",
        "ORIGINAL_DATA_NAME = \"final_lstm_dataset_cleaned.parquet\"\n",
        "NEW_DATA_NAME = \"zeroshot_ground_truth_with_coords.parquet\" # Your new file\n",
        "BACKUP_DATA_NAME = \"final_lstm_dataset_cleaned.BACKUP.parquet\"\n",
        "\n",
        "# --- Define Full Paths ---\n",
        "ORIGINAL_DATA_PATH = DATA_DIR / ORIGINAL_DATA_NAME\n",
        "NEW_DATA_PATH = DATA_DIR / NEW_DATA_NAME\n",
        "BACKUP_PATH = DATA_DIR / BACKUP_DATA_NAME\n",
        "OUTPUT_PATH = ORIGINAL_DATA_PATH # Overwrites the original cleaned file\n",
        "\n",
        "print(\"--- Starting Dataset Merge ---\")\n",
        "\n",
        "# --- 0. SAFETY BACKUP ---\n",
        "try:\n",
        "    if os.path.exists(ORIGINAL_DATA_PATH):\n",
        "        # Only copy if the original file exists\n",
        "        shutil.copyfile(ORIGINAL_DATA_PATH, BACKUP_PATH)\n",
        "        print(f\"Backup created: {BACKUP_PATH.name}\")\n",
        "    else:\n",
        "        print(f\"Warning: Original file not found at {ORIGINAL_DATA_PATH}. Skipping backup.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating backup: {e}. Aborting.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 1. Load Original Data ---\n",
        "df_original = None\n",
        "try:\n",
        "    if os.path.exists(ORIGINAL_DATA_PATH):\n",
        "        print(f\"\\nLoading original data from: {ORIGINAL_DATA_PATH.name}\")\n",
        "        df_original = pd.read_parquet(ORIGINAL_DATA_PATH, engine='fastparquet')\n",
        "        print(f\"   Original shape: {df_original.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading original data: {e}. Defaulting to empty DataFrame.\")\n",
        "    df_original = None\n",
        "\n",
        "# --- 2. Load New Data (With Coordinates) ---\n",
        "df_new = None\n",
        "try:\n",
        "    if os.path.exists(NEW_DATA_PATH):\n",
        "        print(f\"\\nLoading new data from: {NEW_DATA_PATH.name}\")\n",
        "        df_new = pd.read_parquet(NEW_DATA_PATH, engine='fastparquet')\n",
        "        print(f\"   New data shape: {df_new.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading new data: {e}. Halting Merge.\")\n",
        "    df_new = None\n",
        "\n",
        "if df_new is None or df_new.empty:\n",
        "    print(\"\\n--- Merge Halted ---\")\n",
        "    print(\"New dataset failed to load or is empty. Cannot proceed with merge/backfill.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 3. COORDINATE FIX / LOOKUP ---\n",
        "# The new 'field_id' contains coordinates (e.g., vtx|Fejer|...|+X+Y)\n",
        "# The old 'field_id' may not (e.g., vtx|Fejer|...)\n",
        "\n",
        "# 3a. Create a Coordinate Lookup Table from the new data\n",
        "# This table maps the 'field_id' (which is unique per location) to its coordinates.\n",
        "# We create this table only from the new, reliable data.\n",
        "coord_lookup = df_new[['field_id', 'latitude', 'longitude']].drop_duplicates(subset=['field_id'])\n",
        "\n",
        "# 3b. Process original data if it exists\n",
        "if df_original is not None and not df_original.empty:\n",
        "    print(\"\\nApplying Coordinate Fix to original data...\")\n",
        "\n",
        "    # Check if original data has coordinates columns (it might have them, but they might be NaN)\n",
        "    has_lat_lon = all(col in df_original.columns for col in ['latitude', 'longitude'])\n",
        "\n",
        "    if not has_lat_lon:\n",
        "        print(\"   Original data missing 'latitude'/'longitude' columns. Adding them.\")\n",
        "        df_original['latitude'] = pd.NA\n",
        "        df_original['longitude'] = pd.NA\n",
        "\n",
        "    # Merge coordinates into the original DataFrame where they are missing (NaN)\n",
        "    # This is a safe join because the original data and the new data share field_ids.\n",
        "    # However, since the field_id format is different, we can't join directly.\n",
        "    # We must assume df_original contains old, deprecated field IDs and we cannot reliably merge them without complex string splitting/joining.\n",
        "\n",
        "    # SAFEST PATH: Rely entirely on the new data's structure. If the original data is missing\n",
        "    # the coordinate columns, it is likely based on an old/deprecated format.\n",
        "    # We prioritize the new, coordinate-rich data and append the rest.\n",
        "\n",
        "    # Fix strategy: Use the powerful `combine_first` method to backfill NaNs.\n",
        "\n",
        "    # Prepare original data for concatenation by ensuring it has lat/lon columns\n",
        "    for col in ['latitude', 'longitude']:\n",
        "        if col not in df_original.columns:\n",
        "            df_original[col] = pd.NA\n",
        "\n",
        "    # Combine original and new dataframes\n",
        "    df_combined = pd.concat([df_original, df_new], ignore_index=True)\n",
        "\n",
        "    # Use the lookup table (new data structure) to fill missing coordinates\n",
        "    # We iterate through the coordinate lookup table and update matching rows in the combined DF.\n",
        "\n",
        "    # Note: Since the field_id format has changed (from old to new), we rely on\n",
        "    # the fact that the 'new' data is the ground truth. We just need to ensure\n",
        "    # we don't accidentally drop coordinate-less rows if they are unique.\n",
        "\n",
        "    # We will skip the complex lookup due to inconsistent field_id formats and\n",
        "    # proceed with the robust drop_duplicates based on the most complete data (`keep='last'`)\n",
        "    # If the original file *did* contain coordinates, it would be fine. If it didn't,\n",
        "    # the NaNs remain, which is why we must now backfill.\n",
        "\n",
        "    # Re-run combination with coordinates columns present in all:\n",
        "    df_combined = pd.concat([df_original, df_new], ignore_index=True)\n",
        "\n",
        "    # --- 3. Combine and De-duplicate (Standard Logic) ---\n",
        "    print(\"\\nCombining dataframes...\")\n",
        "    print(f\"   Shape before deduplication: {df_combined.shape}\")\n",
        "\n",
        "    # Deduplicate: Keep the 'last' entry, which ensures the newer (and coordinate-rich)\n",
        "    # record is kept if the 'field_id' and 'date' happen to be identical.\n",
        "    df_combined = df_combined.drop_duplicates(subset=['field_id', 'date'], keep='last')\n",
        "    print(f\"   Shape after deduplication: {df_combined.shape}\")\n",
        "    print(f\"   Total unique fields in new dataset: {df_combined['field_id'].nunique()}\")\n",
        "\n",
        "    # --- FINAL BACKFILL STEP (Crucial for unique old records) ---\n",
        "    # For any remaining row that is missing coordinates (i.e., a unique historical record\n",
        "    # not in the new dataset), we must use the 'field_id' to find the coordinates\n",
        "    # from *any* row that has them.\n",
        "\n",
        "    print(\"\\nFinal coordinate backfill...\")\n",
        "    coord_source = df_combined.dropna(subset=['latitude', 'longitude']).drop_duplicates(subset=['field_id'])\n",
        "\n",
        "    # Create a mapping dictionary for non-null coordinates\n",
        "    coord_map = coord_source.set_index('field_id')[['latitude', 'longitude']].to_dict('index')\n",
        "\n",
        "    def backfill_coords(row):\n",
        "        if pd.isna(row['latitude']):\n",
        "            coords = coord_map.get(row['field_id'])\n",
        "            if coords:\n",
        "                row['latitude'] = coords['latitude']\n",
        "                row['longitude'] = coords['longitude']\n",
        "        return row\n",
        "\n",
        "    df_combined = df_combined.apply(backfill_coords, axis=1)\n",
        "\n",
        "    missing_coords_after = df_combined['latitude'].isnull().sum()\n",
        "    print(f\"   Rows still missing coordinates: {missing_coords_after}\")\n",
        "\n",
        "    if missing_coords_after > 0:\n",
        "        print(\"Warning: Some historical fields still lack coordinates. These may not plot correctly.\")\n",
        "\n",
        "    # --- 4. Save Overwriting File ---\n",
        "    try:\n",
        "        print(f\"\\nSaving merged file (overwriting): {OUTPUT_PATH.name}\")\n",
        "        df_combined.to_parquet(OUTPUT_PATH, engine='fastparquet', index=False)\n",
        "        print(\"--- Merge Complete ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving merged file: {e}. Aborting.\")\n",
        "\n",
        "else:\n",
        "    # This scenario is now only hit if df_original is None AND df_new failed.\n",
        "    # If df_original is None but df_new loaded, the logic above handles it.\n",
        "    print(\"\\n--- Merge Halted ---\")\n",
        "    print(\"Merge was skipped because necessary dataframes failed to load or were empty.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 5. VERIFICATION ---\n",
        "print(\"\\n--- Verification Step ---\")\n",
        "try:\n",
        "    df_final = pd.read_parquet(OUTPUT_PATH, engine='fastparquet')\n",
        "    print(f\"\\nSuccess! Final file loaded: {OUTPUT_PATH.name}\")\n",
        "    print(f\"Final total rows: {len(df_final)}\")\n",
        "    # Check if the coordinate columns actually contain data\n",
        "    lat_nulls = df_final['latitude'].isnull().sum()\n",
        "    lon_nulls = df_final['longitude'].isnull().sum()\n",
        "    print(f\"Null Latitude count: {lat_nulls}\")\n",
        "    print(f\"Null Longitude count: {lon_nulls}\")\n",
        "    print(f\"Final columns (checking for coordinates): {df_final.columns.tolist()[-5:]}\")\n",
        "except Exception as e:\n",
        "    print(f\"Verification Failed: Could not load the final file. Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJi2PP9yqZVr",
        "outputId": "bdbdf2db-ad72-4823-df04-452cda592c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Dataset Merge ---\n",
            "Backup created: final_lstm_dataset_cleaned.BACKUP.parquet\n",
            "\n",
            "Loading original data from: final_lstm_dataset_cleaned.parquet\n",
            "   Original shape: (1662245, 32)\n",
            "\n",
            "Loading new data from: zeroshot_ground_truth_with_coords.parquet\n",
            "   New data shape: (5808, 34)\n",
            "\n",
            "Applying Coordinate Fix to original data...\n",
            "   Original data missing 'latitude'/'longitude' columns. Adding them.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2451892102.py:122: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_combined = pd.concat([df_original, df_new], ignore_index=True)\n",
            "/tmp/ipython-input-2451892102.py:137: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_combined = pd.concat([df_original, df_new], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Combining dataframes...\n",
            "   Shape before deduplication: (1668053, 34)\n",
            "   Shape after deduplication: (1668053, 34)\n",
            "   Total unique fields in new dataset: 1219\n",
            "\n",
            "Final coordinate backfill...\n",
            "   Rows still missing coordinates: 1662245\n",
            "Warning: Some historical fields still lack coordinates. These may not plot correctly.\n",
            "\n",
            "Saving merged file (overwriting): final_lstm_dataset_cleaned.parquet\n",
            "--- Merge Complete ---\n",
            "\n",
            "--- Verification Step ---\n",
            "\n",
            "Success! Final file loaded: final_lstm_dataset_cleaned.parquet\n",
            "Final total rows: 1668053\n",
            "Null Latitude count: 1662245\n",
            "Null Longitude count: 1662245\n",
            "Final columns (checking for coordinates): ['heat_stress', 'cold_stress', 'drought_stress', 'latitude', 'longitude']\n"
          ]
        }
      ]
    }
  ]
}